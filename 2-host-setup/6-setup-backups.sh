#!/bin/bash
#
# Phase 6: Set up automated daily backups to Google Cloud Storage.

source "$(dirname "$0")/common.sh"

# --- Constants ---
BACKUP_SCRIPT_PATH="/usr/local/bin/backup-to-gcs.sh"

# --- Main Logic ---
main() {
    log_info "--- Phase 6: Setting up Automated Backups ---"
    ensure_root

    # --- Validate Arguments ---
    if [[ "$#" -ne 2 ]]; then
        log_error "Usage: $0 <GCS_BUCKET_NAME> <BACKUP_DIRECTORY>"
        exit 1
    fi

    local BUCKET_NAME="$1"
    local BACKUP_DIR="$2"

    if [[ -z "${BUCKET_NAME}" ]]; then
        log_error "Bucket name cannot be empty."
        exit 1
    fi

    if [[ ! -d "${BACKUP_DIR}" ]]; then
        log_error "Directory '${BACKUP_DIR}' does not exist. Please create it first."
        exit 1
    fi

    # --- Dependencies ---
    if ! command -v gsutil &> /dev/null; then
        log_warn "gsutil command not found. Installing Google Cloud SDK..."
        apt-get update -qq
        apt-get install -y -qq apt-transport-https ca-certificates gnupg
        echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
        apt-get update -qq
        apt-get install -y -qq google-cloud-sdk
        log_success "Google Cloud SDK installed."
    fi

    log_info "Creating backup script at ${BACKUP_SCRIPT_PATH}..."
    
    # Generate the backup script with improved logging and error handling
    cat <<EOF > "${BACKUP_SCRIPT_PATH}"
#!/bin/bash
#
# This script is run by cron to back up a directory to GCS.
# Auto-generated by 6-setup-backups.sh on $(date -u +"%Y-%m-%dT%H:%M:%SZ")

set -euo pipefail

# --- Configuration ---
BUCKET_NAME="${BUCKET_NAME}"
BACKUP_DIR="${BACKUP_DIR}"
BACKUP_FILENAME="backup-\$(date -u +"%Y-%m-%d-%H%M%S").tar.gz"
TEMP_FILE="/tmp/\${BACKUP_FILENAME}"
DAYS_TO_KEEP=7

# --- Logging Helper ---
log() {
    echo "[\$(date -u +"%Y-%m-%dT%H:%M:%SZ")] \$1"
}

# --- 1. Create Archive ---
log "Creating archive of \${BACKUP_DIR}..."
tar -czf "\${TEMP_FILE}" -C "\$(dirname "\${BACKUP_DIR}")" "\$(basename "\${BACKUP_DIR}")"

# --- 2. Upload to GCS ---
log "Uploading \${BACKUP_FILENAME} to gs://\${BUCKET_NAME}..."

# We attempt the upload and capture failure if it happens
if ! gsutil cp "\${TEMP_FILE}" "gs://\${BUCKET_NAME}/"; then
    log "ERROR: Backup upload failed!"
    
    # --- ALERTING SECTION ---
    # You can add a command here to notify you of the failure.
    # Example (Generic Webhook):
    # curl -X POST -H "Content-Type: application/json" -d '{"text": "Backup Failed for \${BUCKET_NAME}"}' https://your-webhook.url
    
    # Exit with error so cron logs it as a failure
    exit 1
fi

# --- 3. Clean up local temp file ---
rm "\${TEMP_FILE}"

# --- 4. Clean up old backups in GCS ---
log "Cleaning up backups older than \${DAYS_TO_KEEP} days..."
gsutil ls -l "gs://\${BUCKET_NAME}" | while read -r _ timestamp _; do
    # Skip non-backup files
    [[ \$REPLY != *"backup-"* ]] && continue

    file_date=\$(date -d "\${timestamp}" +%s)
    cutoff_date=\$(date -d "- \${DAYS_TO_KEEP} days" +%s)

    if (( file_date < cutoff_date )); then
        old_file_url=\$(echo "\$REPLY" | awk '{print \$3}')
        log "Deleting old backup: \${old_file_url}"
        gsutil rm "\${old_file_url}"
    fi
done

log "Backup complete."
EOF

    log_info "Setting permissions for backup script..."
    chmod 700 "${BACKUP_SCRIPT_PATH}"

    log_info "Setting up daily cron job..."
    local cron_log="/var/log/backup.log"
    local cron_cmd="0 3 * * * ${BACKUP_SCRIPT_PATH} >> ${cron_log} 2>&1"
    (crontab -l 2>/dev/null | grep -vF "${BACKUP_SCRIPT_PATH}"; echo "${cron_cmd}") | crontab -

    log_success "Setup complete!"
    log_info "Logs will be available at ${cron_log}."
    log_info "---------------------------------------------"
}

main "$@"